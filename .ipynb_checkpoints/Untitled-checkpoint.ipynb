{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trans_mat_gridworld():\n",
    "  # 5x5 gridworld laid out like:\n",
    "  # 0  1  2  3  4\n",
    "  # 5  6  7  8  9\n",
    "  # ...\n",
    "  # 20 21 22 23 24\n",
    "  # where 24 is a goal state that always transitions to a\n",
    "  # special zero-reward terminal state (25) with no available actions\n",
    "  trans_mat = np.zeros((26,4,26))\n",
    "\n",
    "  # NOTE: the following iterations only happen for states 0-23.\n",
    "  # This means terminal state 25 has zero probability to transition to any state,\n",
    "  # even itself, making it terminal, and state 24 is handled specially below.\n",
    "\n",
    "  # Action 0 = down\n",
    "  for s in range(24):\n",
    "    if s < 20:\n",
    "      trans_mat[s,0,s+5] = 1\n",
    "    else:\n",
    "      trans_mat[s,0,s] = 1\n",
    "\n",
    "  # Action 1 = up\n",
    "  for s in range(24):\n",
    "    if s >= 5:\n",
    "      trans_mat[s,1,s-5] = 1\n",
    "    else:\n",
    "      trans_mat[s,1,s] = 1\n",
    "\n",
    "  # Action 2 = left\n",
    "  for s in range(24):\n",
    "    if s%5 > 0:\n",
    "      trans_mat[s,2,s-1] = 1\n",
    "    else:\n",
    "      trans_mat[s,2,s] = 1\n",
    "\n",
    "  # Action 3 = right\n",
    "  for s in range(24):\n",
    "    if s%5 < 4:\n",
    "      trans_mat[s,3,s+1] = 1\n",
    "    else:\n",
    "      trans_mat[s,3,s] = 1\n",
    "\n",
    "  # Finally, goal state always goes to zero reward terminal state\n",
    "  for a in range(4):\n",
    "    trans_mat[24,a,25] = 1\n",
    "\n",
    "  return trans_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-affd068fb198>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-affd068fb198>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    \"\"\"n_states = np.shape(trans_mat)[0]\u001b[0m\n\u001b[0m                                        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def calcMaxEntPolicy(trans_mat, horizon, r_weights, state_features):\n",
    "  \"\"\"\n",
    "  For a given reward function and horizon, calculate the MaxEnt policy that gives equal weight to equal reward trajectories\n",
    "\n",
    "  trans_mat: an S x A x S' array of transition probabilites from state s to s' if action a is taken\n",
    "  horizon: the finite time horizon (int) of the problem for calculating state frequencies\n",
    "  r_weights: a size F array of the weights of the current reward function to evaluate\n",
    "  state_features: an S x F array that lists F feature values for each state in S\n",
    "\n",
    "  return: an S x A policy in which each entry is the probability of taking action a in state s\n",
    "\n",
    "\n",
    "Algorithm\n",
    "\n",
    "Policy Iteration()\n",
    "    v[s for s in S] = 0\n",
    "    for count < 100\n",
    "        for s in S\n",
    "            for s_prime in S_prime\n",
    "                for all a\n",
    "                    Q(s/a) = Trans-mat[s][s_prime][a]*(reward+(gama*v[s_prime])\n",
    "                choose max from Q(s) =\n",
    "\n",
    "Policy Upadating\n",
    "    for s in S\n",
    "        for s_prime in S_prime\n",
    "            for a in a\n",
    "                 argmax(Q(s,a) // this returns the index\n",
    "\"\"\" \n",
    "    n_states = np.shape(trans_mat)[0]\n",
    "    n_actions = np.shape(trans_mat)[1]\n",
    "    policy = np.zeros((n_states,n_actions))\n",
    "    value_function[n_states] = np.zeros(n_states)\n",
    "    print(value_function)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcExpectedStateFreq(trans_mat, horizon, start_dist, policy):\n",
    "  \"\"\"\n",
    "  Given a MaxEnt policy, begin with the start state distribution and propagate forward to find the expected state frequencies over the horizon\n",
    "\n",
    "  trans_mat: an S x A x S' array of transition probabilites from state s to s' if action a is taken\n",
    "  horizon: the finite time horizon (int) of the problem for calculating state frequencies\n",
    "  start_dist: a size S array of starting start probabilities - must sum to 1\n",
    "  policy: an S x A array array of probabilities of taking action a when in state s\n",
    "\n",
    "  return: a size S array of expected state visitation frequencies\n",
    "  \"\"\"\n",
    "\n",
    "  state_freq = np.zeros(len(start_dist))\n",
    "  return state_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def maxEntIRL(trans_mat, state_features, demos, seed_weights, n_epochs, horizon, learning_rate):\n",
    "  \"\"\"\n",
    "  Compute a MaxEnt reward function from demonstration trajectories\n",
    "\n",
    "  trans_mat: an S x A x S' array that describes transition probabilites from state s to s' if action a is taken\n",
    "  state_features: an S x F array that lists F feature values for each state in S\n",
    "  demos: a list of lists containing D demos of varying lengths, where each demo is series of states (ints)\n",
    "  seed_weights: a size F array of starting reward weights\n",
    "  n_epochs: how many times (int) to perform gradient descent steps\n",
    "  horizon: the finite time horizon (int) of the problem for calculating state frequencies\n",
    "  learning_rate: a multiplicative factor (float) that determines gradient step size\n",
    "\n",
    "  return: a size F array of reward weights\n",
    "  \"\"\"\n",
    "\n",
    "  n_features = np.shape(state_features)[1]\n",
    "  r_weights = np.zeros(n_features)\n",
    "  return r_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6d8974665758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Build domain, features, and demos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrans_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_trans_mat_gridworld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mstate_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Terminal state has no features, forcing zero reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdemos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-7848773aaf39>\u001b[0m in \u001b[0;36mbuild_trans_mat_gridworld\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# where 24 is a goal state that always transitions to a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# special zero-reward terminal state (25) with no available actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mtrans_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# NOTE: the following iterations only happen for states 0-23.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "  # Build domain, features, and demos\n",
    "  trans_mat = build_trans_mat_gridworld()\n",
    "  state_features = np.eye(26,25)  # Terminal state has no features, forcing zero reward\n",
    "  demos = [[0,1,2,3,4,9,14,19,24,25],[0,5,10,15,20,21,22,23,24,25],[0,5,6,11,12,17,18,23,24,25],[0,1,6,7,12,13,18,19,24,25]]\n",
    "  seed_weights = np.zeros(25)\n",
    "\n",
    "  # Parameters\n",
    "  n_epochs = 100\n",
    "  horizon = 10\n",
    "  learning_rate = 1.0\n",
    "\n",
    "  # Main algorithm call\n",
    "  r_weights = maxEntIRL(trans_mat, state_features, demos, seed_weights, n_epochs, horizon, learning_rate)\n",
    "\n",
    "  # Construct reward function from weights and state features\n",
    "  reward_fxn = []\n",
    "  for s_i in range(25):\n",
    "    reward_fxn.append( np.dot(r_weights, state_features[s_i]) )\n",
    "  reward_fxn = np.reshape(reward_fxn, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
